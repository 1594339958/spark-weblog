akka {
  loglevel = INFO
  stdout-loglevel = INFO
  akka.loggers = ["akka.event.slf4j.Slf4jLogger"]
}

actor {
  duration = 10
  retries = 10  
  timeout = 10
}

#
# Access to cassandra is provided by Datastax' spark-cassandra-connector; the respective
# configuration parameters can be retrieved from here: 
#
# https://github.com/datastax/spark-cassandra-connector/blob/master/doc/0_quick_start.md
#
cassandra {
  spark.cassandra.connection.host="127.0.0.1"
}

input {
  path=""
}

logfile {

  # whitespace delimiter
  field.delim="\\s+"
  # reference to field positions after having split a log line
  # into fields
  field.meta="cookie:2,date:0,time:1,url:3,referrer:4"
  
  user.id.name="__RequestVerificationToken_Lw__"
  session.id.name=".ASPXAUTH"  
  cookie.delim=";\\+"
  
  # configurable sequence of web paths
  flow.sequence="/shoppingCart,/checkOut,/signin,/signup,/billing,/confirmShipping,/placeOrder"
 
}

mongo {
  mongo.input.uri="mongodb://127.0.0.1:27017/beowulf.input"
}

output {
  # the directory where all web-log data are 
  # read from and written to
  path="/Work/tmp/web-log"

}
redis {
  host="127.0.0.1"
  port="6379"
}

rest {
  host="127.0.0.1"
  port=9000
}

spark {
  spark.executor.memory="1g"
  spark.kryoserializer.buffer.mb="256"
}
