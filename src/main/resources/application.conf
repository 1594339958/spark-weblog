akka {
  loglevel = INFO
  stdout-loglevel = INFO
  akka.loggers = ["akka.event.slf4j.Slf4jLogger"]
}

actor {
  duration = 10
  retries = 10  
  timeout = 10
}

# Access parameters to order and item related data
# in an Elasticsearch index; note, that the fields
# must be comaptible to the field specification
elastic {
  es.nodes="localhost"
  es.port="9200"
  es.resource=""                
  es.query=""                          
}

file {
  path=""
}

markov {
  # Path to the directory where all models are saved
  base=""
}

mysql {
  url="127.0.0.1:8889"
  database="analytics"    
  user="root"
  password="root" 
}

redis {
  host="127.0.0.1"
  port="6379"
}
#
# Configuration parameters for the REST API
# of the Intent Recognition Engine
#
rest {
  host="127.0.0.1"
  port=9000
}

spark {
  spark.executor.memory="1g"
  spark.kryoserializer.buffer.mb="256"
}

##################

insight {
  time = 10
}

router {
  retries = 10
  time    = 10
  workers = 10
}

logfile {

  # whitespace delimiter
  field.delim="\\s+"
  # reference to field positions after having split a log line
  # into fields
  field.meta="cookie:2,date:0,time:1,url:3,referrer:4"
  
  user.id.name="__RequestVerificationToken_Lw__"
  session.id.name=".ASPXAUTH"  
  cookie.delim=";\\+"
  
  pagetime.rating="10:1,20:2,35:3,55:4,80:5,120:6"
  
  # configurable sequence of web paths
  flow.sequence="/shoppingCart,/checkOut,/signin,/signup,/billing,/confirmShipping,/placeOrder"
 
}

conversion {
  path = "/Work/tmp/web-log/goals.xml"
}

mining {

  # the directory where all web-log data are 
  # read from and written to
  path="/Work/tmp/web-log/"

}